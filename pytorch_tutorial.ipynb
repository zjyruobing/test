{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "pytorch_tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zjyruobing/test/blob/master/pytorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n",
          "is_executing": false
        },
        "id": "n3z95OelICk8",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "hiZt_-hoIClC",
        "colab_type": "text"
      },
      "source": [
        "The goal of this tutorial is to give students a formal introduction to PyTorch and the functionality that the library provides to students seeking to build and test Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-jK0zGmoIClF",
        "colab_type": "text"
      },
      "source": [
        "## Importing PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Ew4n3wuSIClI",
        "colab_type": "text"
      },
      "source": [
        "This tutorial assumes a working installation of PyTorch is already available to you. If you don't have one, you should be able to run 'pip install pytorch' if you have a python installation. If you have conda/anaconda installed you can run 'conda install pytorch'.\n",
        "\n",
        "To get started we simply import torch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "z-mj_Pb7IClL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "ADdLNYiAIClW",
        "colab_type": "code",
        "outputId": "57a7f82e-3a50-45a7-ee10-fd00e6f1e849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('PyTorch Version:', torch.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version: 1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Ro0QNkHUICle",
        "colab_type": "text"
      },
      "source": [
        "Documentation can be found for PyTorch at [pytorch.org](https://pytorch.org/docs/). \n",
        "\n",
        "It can take some time to learn your way around but most things you will use can\n",
        "be found in either torch, torch.nn, torch.Tensor, and torch.optim. \n",
        "\n",
        "Now lets import some other useful tools in the library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "146q4_vxIClh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "RDONBwKOICln",
        "colab_type": "text"
      },
      "source": [
        "You might have noticed that we imported NumPy as one of the useful libraries. PyTorch shares a lot of functionality with NumPy with quick and easy transitions from torch Tensors to NumPy arrays.\n",
        "\n",
        "If you have some experience with NumPy you can think of PyTorch as an expansion on top of NumPy that allows you to easily build classifiers without having to manually calculate loss or derivatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "x11gdBiJIClq",
        "colab_type": "text"
      },
      "source": [
        "## Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "sWldzeSNICls",
        "colab_type": "text"
      },
      "source": [
        "Tensors are a collection of numbers represented as an array.\n",
        "\n",
        " - A scalar is a single number. \n",
        " \n",
        " 1\n",
        " \n",
        " - A row vector is a vector $v$ with of dimensions $1 \\times d_c$.\n",
        " \n",
        "    [1, 1]\n",
        " \n",
        " - A column vector is a vector $x$ with dimensions $d_r \\times 1$.\n",
        " \n",
        " [[1],\n",
        " \n",
        "  [1]]\n",
        " - A matrix can be thought of as a combination of the above two. The matrix $z$ is $d_r \\times d_c$.\n",
        " \n",
        " [[1, 1],\n",
        " \n",
        "  [1, 1]]\n",
        "\n",
        "- A tensor $t$ is a cube of dimensionality $d_r \\times d_c \\times d_d$.\n",
        "\n",
        " [[[1, 1], [1, 1]]\n",
        "\n",
        " [[1, 1], [1, 1]]]\n",
        " \n",
        " Vectors, Matrices, and Tensors are the building blocks off all operations that occur within PyTorch.\n",
        " The flow of operations and the predictions that you get are the result of you chaining together matrix multiplication.\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nohX56FVIClw",
        "colab_type": "text"
      },
      "source": [
        "## Creating Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "dA2o4nutICly",
        "colab_type": "text"
      },
      "source": [
        "Tensors can be created in a dozens of ways (all of which are listed in the documentation), but we will only be discussing a few."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "ZAgdfqHzICl2",
        "colab_type": "code",
        "outputId": "42d41565-fdf2-42f8-8823-d3bbf02cccfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "x = torch.Tensor(2,4)\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.0533e-36, 0.0000e+00, 4.4842e-44, 0.0000e+00],\n",
            "        [       nan, 0.0000e+00, 2.1276e+23, 6.7013e-10]])\n",
            "torch.Size([2, 4])\n",
            "torch.Size([2, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uBMSywZ6ICl8",
        "colab_type": "text"
      },
      "source": [
        "So we created a tensor, printed its output, and its size. If you run the above command a few times, you may find that your tensor isn't always filled with zeros. Any idea why that might be?\n",
        "\n",
        "To fix this problem we run the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "M-r1iBKwICl-",
        "colab_type": "code",
        "outputId": "1f32c52f-21d0-4cbd-a0b9-48dff13ae2f3",
        "colab": {}
      },
      "source": [
        "x.zero_()\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiPlj8wMJC0M",
        "colab_type": "text"
      },
      "source": [
        "Or you can do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usV-ItucJFNR",
        "colab_type": "code",
        "outputId": "dbb618b6-1205-4c8a-c0a8-b65fcc83e515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x = torch.zeros((2,4))\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "heTipSOUICmF",
        "colab_type": "text"
      },
      "source": [
        "You might have also noticed that we used the .shape or .size() calls to get the shape of the tensor.\n",
        "\n",
        "When you do this, you'll find that the dimensions are indexed, like in a list, and can be referenced like we do below: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "vRbe3dYvICmG",
        "colab_type": "code",
        "outputId": "2968d3dd-38ba-401b-a82c-a2b8822e0385",
        "colab": {}
      },
      "source": [
        "print(x.shape)\n",
        "print(x.shape[0])\n",
        "print(x.shape[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4])\n",
            "2\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp4MrmoDJq7l",
        "colab_type": "text"
      },
      "source": [
        "## Data Types in PyTorch\n",
        "\n",
        "|Type\t|Function Call   \t|Comments   \t|   \t|   \t|\n",
        "|---\t|---\t|---\t|---\t|---\t|\n",
        "|32-bit Float  \t|torch.FloatTensor()   \t|Normally used for storing inputs, weights, and gradients. GPUs are optimized for this data type.\n",
        "|64-bit Float  \t|torch.DoubleTensor()   \t|Used when single precision floats are too imprecise and round off. Training times are significantly slower|\n",
        "|16-bit Float  \t|torch.HalfTensor()   \t|Can be used for storing inputs, weights, and gradients. Suffer from rounding problems. Tend to train faster on GPU than 32-bit floats|\n",
        "|64-bit Integer\t|torch.LongTensor()|Normally used to store Y values for when you calculate loss.\t|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n",
          "is_executing": false
        },
        "id": "nGZKV1HQICmM",
        "colab_type": "text"
      },
      "source": [
        "Creating a float tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "GqvmsYu8ICmO",
        "colab_type": "code",
        "outputId": "808b7815-9646-435d-d9cc-6eec95a16271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "x = torch.FloatTensor(2, 2)\n",
        "print(x)\n",
        "print(x.type())\n",
        "x = torch.Tensor(2, 2)\n",
        "print(x)\n",
        "print(x.type())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.0541e-36, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00]])\n",
            "torch.FloatTensor\n",
            "tensor([[2.0541e-36, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00]])\n",
            "torch.FloatTensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "HfVi2kw7ICmV",
        "colab_type": "text"
      },
      "source": [
        "Creating a long tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "9uxMDmkNICmY",
        "colab_type": "code",
        "outputId": "3282481a-f463-4abf-ba48-7c54951078d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "x = torch.LongTensor(2, 2)\n",
        "print(x)\n",
        "print(x.type())\n",
        "x = torch.Tensor(2, 2).long()\n",
        "print(x)\n",
        "print(x.type())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[           70172032,                  32],\n",
            "        [         4294967295, 3474584533648815920]])\n",
            "torch.LongTensor\n",
            "tensor([[0, 0],\n",
            "        [0, 0]])\n",
            "torch.LongTensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUFlxQB0MnR8",
        "colab_type": "text"
      },
      "source": [
        "Remember LongTensors store integers and are used for storing your Y values. That's why the values they store don't have decimals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "hq2S81PrICmd",
        "colab_type": "text"
      },
      "source": [
        "If we need to get information from a list or a NumPy array, we can easily convert them into torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "X2Q6XJpMICmf",
        "colab_type": "code",
        "outputId": "128b4473-6bc0-4610-a2f1-23cc1aa4876d",
        "colab": {}
      },
      "source": [
        "x = [[1,2], [3 ,4]]\n",
        "print(x)\n",
        "matrix = np.array(x)\n",
        "print(matrix)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2], [3, 4]]\n",
            "[[1 2]\n",
            " [3 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TS_rUtrlICmk",
        "colab_type": "text"
      },
      "source": [
        "Of course, we can go directly to a torch tensor from a list too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "EVJC_Dc3ICml",
        "colab_type": "code",
        "outputId": "00b2bb10-c741-4603-e82e-1ca53d679000",
        "colab": {}
      },
      "source": [
        "x = [[1,2], [3 ,4]]\n",
        "print(x)\n",
        "matrix = torch.FloatTensor(x)\n",
        "print(matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2], [3, 4]]\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "aFNPbrrWICmp",
        "colab_type": "text"
      },
      "source": [
        "Here are a few other ways to create tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "qsJAz87SICms",
        "colab_type": "code",
        "outputId": "d18f0896-408e-40b1-c1f9-2d2693062265",
        "colab": {}
      },
      "source": [
        "t_zeros = torch.zeros((2,3))\n",
        "t_ones = torch.ones(2, 3)            # creates a tensor with 1s\n",
        "t_fives = torch.empty(2, 3).fill_(5) # creates a non-initialized tensor and fills it with 5\n",
        "t_random = torch.rand(2, 3)          # creates a uniform random tensor\n",
        "t_normal = torch.randn(2, 3)         # creates a normal random tensor\n",
        "\n",
        "print(t_zeros)\n",
        "print()\n",
        "print(t_ones)\n",
        "print()\n",
        "print(t_fives)\n",
        "print()\n",
        "print(t_random)\n",
        "print()\n",
        "print(t_normal)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n",
            "\n",
            "tensor([[0.2184, 0.8291, 0.6408],\n",
            "        [0.9656, 0.9591, 0.9560]])\n",
            "\n",
            "tensor([[ 0.4583,  0.8787, -0.0888],\n",
            "        [-2.5900, -1.5812,  0.1675]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "sD3pr2cvICmy",
        "colab_type": "text"
      },
      "source": [
        "### Converting back to NumPy and Python Lists\n",
        "Earlier we saw that we can easily go from a python list or NumPy array to a PyTorch Tensor, but we didn't discuss how to get back.\n",
        "\n",
        "To go from a tensor back to a NumPy array you can use the below command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "bKT7zKb9ICmz",
        "colab_type": "code",
        "outputId": "bc4f4ba2-75e2-4b40-c8b6-ed5dafc6b7c3",
        "colab": {}
      },
      "source": [
        "print(matrix.numpy())\n",
        "print(type(matrix.numpy()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "[[1. 2.]\n",
            " [3. 4.]]\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "chkf0IXLICm3",
        "colab_type": "text"
      },
      "source": [
        "We can do the same thing but to a list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "tyTTlWHKICm4",
        "colab_type": "code",
        "outputId": "45b6f62b-e4f1-4bbd-bcf0-a1b443727577",
        "colab": {}
      },
      "source": [
        "print(matrix.tolist())\n",
        "print(type(matrix.tolist()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0, 2.0], [3.0, 4.0]]\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "gjoKdvYYICm_",
        "colab_type": "text"
      },
      "source": [
        "Notice that when we did our conversion, the dimensions didn't change as we moved from torch to numpy or torch to a python list.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5qULAB9hICnC",
        "colab_type": "text"
      },
      "source": [
        "### Reshaping Tensors\n",
        "We can reshape tensors in a variety of ways to include adding dimensions, removing dimensions, or changing the size of a dimension.\n",
        "This is particularly useful because some neural network operations expect inputs to be a certain size and will throw errors when the dimensions are incorrect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "ke6mMYT3ICnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.rand(3,3,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "1VnBugs6ICnK",
        "colab_type": "code",
        "outputId": "1ca7d84d-8ea4-4d13-e8d6-9066ab45e863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x.reshape(3,3,1,3).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "WmoJibFPICnO",
        "colab_type": "code",
        "outputId": "bebf3f08-f169-43d1-a6ac-da15b1e4cac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x.reshape(3,3,1,-1).shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2BlgMf-EICnT",
        "colab_type": "code",
        "outputId": "c42403dc-1411-4f27-bac9-04506e5837cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x.reshape(-1).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U75S72npS3R_",
        "colab_type": "text"
      },
      "source": [
        "In one of your assignments this semester you'll use Recurrent Neural Networks to build your system. \n",
        "\n",
        "RNNs in PyTorch expect the input to be:\n",
        "\n",
        "$SentenceLength \\times BatchSize \\times EmbeddingDims$\n",
        "\n",
        "So if you have a single sentence of 5 words and your word embeddings are 10 dimensions you'll have a matrix like this:\n",
        "\n",
        "$5 \\times 10$\n",
        "\n",
        "Since you need a dimension for batch size, you'll want to call .reshape() to make a dimension for $BatchSize$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "105Ld_udRB1c",
        "colab_type": "code",
        "outputId": "6622c05a-71ee-422d-89a7-db05967c220e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x = torch.Tensor(5, 10)\n",
        "print(x.shape)\n",
        "x = x.reshape(5, 1, 10)\n",
        "print(x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 10])\n",
            "torch.Size([5, 1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u4kImGSRNu1",
        "colab_type": "text"
      },
      "source": [
        "Now we can say that its in the format that an RNN can take!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HES0iP7zV2vg",
        "colab_type": "text"
      },
      "source": [
        "#Optimizers\n",
        "\n",
        "Pytorch has dozens of built-in optimization functions for updating your weights as you train. These functions are stored in the torch.optim library with full documentation on each. Lets take a look at [Stochastic Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD).\n",
        "\n",
        "There are often many different settings that optimizers take as input but we normally use them like this:\n",
        "\n",
        "```\n",
        "import torch.optim as optim\n",
        "model = model() # create the model that we're training\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1) # give the optimizer the weights of the model\n",
        "```\n",
        "\n",
        "The mathematics behind each optimizer is out of the scope of this tutorial but I'd encourage you to research their differences and understand why some work better than others.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGt4jmAUXdfE",
        "colab_type": "text"
      },
      "source": [
        "# Loss Functions\n",
        "\n",
        "If you haven't dealt with loss functions in the past you can think of them as functions that quantify how wrong your algorithm is compared to what it should predict.\n",
        "\n",
        "Sum-of-Squares, or Mean Squared Error, is often used with Linear Regression. This loss is defined as such:\n",
        "\n",
        "$ \\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2$\n",
        "\n",
        "In PyTorch we can use the torch.nn.MSELoss() function instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhW53-igqnO1",
        "colab_type": "code",
        "outputId": "7d625570-1965-4191-bb3d-f1a9698f9a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y = torch.LongTensor([2])\n",
        "yhat = torch.FloatTensor([0])\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "print(loss(y,yhat))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D2o2cZlq-CL",
        "colab_type": "text"
      },
      "source": [
        "Of course, we aren't limited to Sum-of-Squares error. There are tons that we have available in PyTorch's library.\n",
        "\n",
        "While Sum-of-Squares is good for regression, most of your work in this class will be multi-class classification problems.\n",
        "\n",
        "Cross Entropy Loss (torch.nn.CrossEntropyLoss()) will be a good loss function to use for this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skFqqLM6qydb",
        "colab_type": "code",
        "outputId": "51f19247-f748-4de6-c9a6-5afef0805acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "y = torch.LongTensor([1])\n",
        "print(y.shape)\n",
        "yhat = torch.FloatTensor([[1,0]])\n",
        "print(yhat.shape)\n",
        "\n",
        "# in_ = torch.randn(3, 5, requires_grad=True)\n",
        "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "print(loss(yhat,y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1])\n",
            "torch.Size([1, 2])\n",
            "tensor(1.3133)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMirNMnawh1c",
        "colab_type": "text"
      },
      "source": [
        "**Note:** Pay close attention to the order that you provide y and yhat to the loss function. Some functions use y, yhat while other take yhat, y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WgsBOKyw6Fq",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks\n",
        "At long last, we are finally talking about building neural networks!\n",
        "\n",
        "All models/networks in PyTorch will have the nn.Module object as its parent class. This provides your model with all of PyTorch's built-in math operations.\n",
        "\n",
        "To build a class you'll want to name it, initialize all layers in the __init__() function and then define your algorithm in the forward() function.\n",
        "\n",
        "A model for Logistic Regression can be found below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpDzrktHyBE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.L1 = nn.Linear(dims,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.L1(x)\n",
        "        a1 = self.sigmoid(z1)\n",
        "        return a1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRJx0Ygmygzb",
        "colab_type": "text"
      },
      "source": [
        "To use this network, we initialize it and call the forward function with a tensor as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22VQuUTIyP14",
        "colab_type": "code",
        "outputId": "6825fe69-078a-4984-ad7e-90c5df0e5954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = LogisticRegression(10)\n",
        "x = torch.rand(1,10)\n",
        "\n",
        "yhat = model.forward(x)\n",
        "yhat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5566]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXuvN6WSy3xc",
        "colab_type": "text"
      },
      "source": [
        "A Feed Forward Neural Network can be defined very similarly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by-jiZJ5yaNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNeuralNetwork(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super(FeedForwardNeuralNetwork, self).__init__()\n",
        "        self.L1 = nn.Linear(dims,5)\n",
        "        self.L2 = nn.Linear(5, 3)\n",
        "        self.L3 = nn.Linear(3, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        a1 = self.sigmoid(self.L1(x))\n",
        "        a2 = self.sigmoid(self.L2(a1))\n",
        "        a3 = self.sigmoid(self.L3(a2))\n",
        "        return a3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwG7MmY8zBYK",
        "colab_type": "code",
        "outputId": "d677b733-513d-438b-bb6c-20d792a76fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = FeedForwardNeuralNetwork(10)\n",
        "x = torch.rand(1,10)\n",
        "\n",
        "yhat = model.forward(x)\n",
        "yhat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3282]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgdRjO2_0Qpz",
        "colab_type": "text"
      },
      "source": [
        "As well as a Recurrent Neural Network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xehbLwRqzDf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RecurrentNeuralNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(RecurrentNeuralNetwork, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=10) # Many other kwargs are available for nn.RNN. Number of layers, which activation function, dropout, uni or bidirectional\n",
        "        self.linear = nn.Linear(10, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Remember, RNNs assume the input has this shape: [sequence_length, batch_size, embedding_dim]\n",
        "        out, _ = self.rnn(x)\n",
        "        return self.sigmoid(self.linear(out[0,0,:]))  # Use Logistic Regression to get a probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wY-pkXU1uxI",
        "colab_type": "text"
      },
      "source": [
        "# Document Classification Using Logistic Regression\n",
        "\n",
        "First, we import all of the libraries that we are going to need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlXHDryk1y6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import CountVectorizer # This is to create a Bag-of-Words representation for each sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-IWORhG2FKx",
        "colab_type": "text"
      },
      "source": [
        "Next, we create the documents, or sentences, that we're going to train on. Then convert them to their Bag-of-Words representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxsJS9nj137Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = ['i love my cat',\n",
        "             'dogs are the best',\n",
        "             'my cat is always knocking things over',\n",
        "             'dogs are man\\'s best friend']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit_transform(sentences)\n",
        "x = bow.toarray()\n",
        "\n",
        "x = torch.Tensor(x).cuda()  # Tensor of shape 4x14\n",
        "y = torch.Tensor([[1], [0], [1], [0]]).cuda()  # Tensor of shape 4x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWf-WCcC2Y3r",
        "colab_type": "code",
        "outputId": "9dcef183-9f8f-4778-b0e5-d353742706c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(vectorizer.get_feature_names())\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['always', 'are', 'best', 'cat', 'dogs', 'friend', 'is', 'knocking', 'love', 'man', 'my', 'over', 'the', 'things']\n",
            "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
            "        [0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.],\n",
            "        [0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k7HI16q39zF",
        "colab_type": "text"
      },
      "source": [
        "Now we create our Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJmiwKsg3PM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNeuralNetwork(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super(FeedForwardNeuralNetwork, self).__init__()\n",
        "        self.L1 = nn.Linear(dims,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.L1(x)\n",
        "        a1 = self.sigmoid(z1)\n",
        "        return a1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPguVMiC4Bu3",
        "colab_type": "text"
      },
      "source": [
        "Lastly, our training loop for training our system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnu4Zktg4LJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = FeedForwardNeuralNetwork(x.shape[1])  # Logistic Regression with x.shape[1] dimensions\n",
        "model.to('cuda')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Optimizing with Stochastic Gradient Descent\n",
        "loss = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "for epoch in range(1000):  # Training Loop\n",
        "    model.zero_grad()  # Zero all the Gradients\n",
        "    yhat = model.forward(x)  # Compute forward pass\n",
        "    output = loss(yhat, y)  # Compute loss\n",
        "    output.backward()  # Back propagate loss\n",
        "    optimizer.step()  # Update weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9LAPn404Mam",
        "colab_type": "code",
        "outputId": "b09fd7bd-cd3e-4ad0-b778-08296b8c75ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "print(model.forward(x))\n",
        "print(vectorizer.get_feature_names())\n",
        "print(model.L1.weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9894],\n",
            "        [0.0066],\n",
            "        [0.9971],\n",
            "        [0.0049]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "['always', 'are', 'best', 'cat', 'dogs', 'friend', 'is', 'knocking', 'love', 'man', 'my', 'over', 'the', 'things']\n",
            "Parameter containing:\n",
            "tensor([[ 0.5516, -1.4384, -1.4364,  1.4082, -1.3732, -0.6979,  0.3297,  0.4618,\n",
            "          1.0960, -0.5583,  1.8470,  0.5741, -0.9420,  0.4994]],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}